# Speaker Recognition using Pitch, MFCC and KNN classifier
This project uses machine learning approach to recognise speaker based on features extracted from recorded speech. Initial idea was taken from an [MathWorks article](https://www.mathworks.com/help/audio/examples/speaker-identification-using-pitch-and-mfcc.html) where they demonstrated the method using different MATLAB toolboxes, I avoided using those toolbox and tried to implement most of the algorithms from scratch.  

## Brief Introduction of the Algorithm:
I extracted features from voice which is fundamental to every speaker. Some of these features are pitch and MFCC (Mel-frequency cepstral coefficients). I extracted these features from voice data and stored them to use later for training in machine learning algorithm. The algorithm that was used in order to classify speaker is K nearest neighbor(KNN). This is a supervised classifier algorithm. Features extracted are used in KNN to make the training data. Then when new user input (speech) is given, KNN algorithm identifies the speaker based on the training data. 

<img src="https://www.mathworks.com/help/examples/audio_wavelet/win64/xxSpeakerID01.png" width="600">

### How KNN Algorithm Works:
Every supervised classifiers need some features for every sample data (training or testing). Let us say, a similar type of problem where whether a patient has cancer or not, has to be detected. Each patient data has two features by which we can decide whether he has cancer or not. Then we can plot every train data, where x and y axis represent two feature values. If the red samples are positive and green are negative then it should look something like below.


<img src="https://github.com/NadimC137/SpeakerRecognitionKNN/blob/master/images/knn1.png" width="600">

Now we want to detect a patient’s condition, whose data has been plotted using star sign. We intend to find out the class of the star. It can be green or red. The “K” in KNN algorithm is the nearest neighbors we wish to take vote from. Let’s say K = 3. Hence, we will now make a circle with the star as center just as big as to enclose only three data points on the plane. Refer to following diagram to get an idea:

<img src="https://github.com/NadimC137/SpeakerRecognitionKNN/blob/master/images/knn2.png" width="600">

The three closest points to the star is all Red. Hence, with good confidence level we can say that the star should belong to the class Red. That is, the patient has cancer. This is k nearest neighbor algorithm.


### Feature Extraction:
We will extract two features from voice. They are-
*	Pitch
*	MFCC (Mel-frequency cepstral coefficients).

### Pitch:
Pitch is frequency domain analysis of voice data, yet it is not exactly Fourier transform, because in pitch, we take a short time frame (window) and do the FFT of that portion, then go to next time frame and do the same thing. Thus we get a frequency response depending on time. This is important in our case, because we want to extract frequencies when speaker is talking and that should be variable frequency depending on time. Pitch analysis of voice data looks something like this:

<img src="https://github.com/NadimC137/SpeakerRecognitionKNN/blob/master/images/pitch1.png" width="800">

### MFCC (Mel-frequency cepstral coefficients):
The main point to understand about speech is that the sounds generated by a human are filtered by the shape of the vocal tract including tongue, teeth etc. This shape determines what sound comes out. If we can determine the shape accurately, this should give us an accurate representation of the phoneme being produced. The shape of the vocal tract manifests itself in the envelope of the short time power spectrum, and the job of MFCCs is to accurately represent this envelope. There is an amazing article about MFCC in medium [The dummy’s guide to MFCC](https://medium.com/prathena/the-dummys-guide-to-mfcc-aceab2450fd) which was very helpful when I first started with this project.

Cepstral is spectral with the spec reversed. There is a reason behind this naming. For a very basic understanding, cepstrum is the information of rate of change in spectral bands. In the conventional analysis of time signals, any periodic component shows up as sharp peaks in the corresponding frequency spectrum (Fourier spectrum). This is obtained by applying a Fourier transform on the time signal.
On taking the log of the magnitude of this Fourier spectrum, and then again taking the spectrum of this log by a cosine transformation, we observe a peak wherever there is a periodic element in the original time signal. Since we apply a transform on the frequency spectrum itself, the resulting spectrum is neither in the frequency domain nor in the time domain and hence it was decided to call this the quefrency domain. And this spectrum of the log of the spectrum of the time signal was named cepstrum. The following image is a summary of the above explained steps.

<img src="https://github.com/NadimC137/SpeakerRecognitionKNN/blob/master/images/mfcc1.png" width="600">

Pitch is one of the characteristics of a speech signal and is measured as the frequency of the signal. Mel scale is a scale that relates the perceived frequency of a tone to the actual measured frequency. It scales the frequency in order to match more closely what the human ear can hear (humans are better at identifying small changes in speech at lower frequencies). This scale has been derived from sets of experiments on human subjects. Let me give you an intuitive explanation of what the mel scale captures.
The range of human hearing is 20 Hz to 20 kHz. Imagine a tune at 300 Hz. This would sound something like the standard dialer tone of a land-line phone. Now imagine a tune at 400 Hz (a little higher pitched dialer tone). Now compare the distance between these two howsoever this may be perceived by your brain. Now imagine a 900 Hz signal (similar to a microphone feedback sound) and a 1 kHz sound. The perceived distance between these two sounds may seem greater than the first two although the actual difference is the same (100Hz). The mel scale tries to capture such differences. A frequency measured in Hertz (f) can be converted to the Mel scale using the following formula:

<img src="https://github.com/NadimC137/SpeakerRecognitionKNN/blob/master/images/mfcc2.png" width="600">

Any sound generated by humans is determined by the shape of their vocal tract (including tongue, teeth, etc.) If this shape can be determined correctly, any sound produced can be accurately represented. The envelope of the time power spectrum of the speech signal is representative of the vocal tract and MFCC (which is nothing but the coefficients that make up the Mel-frequency cepstrum) accurately represents this envelope. The following block diagram is a step-wise summary of how we arrived at MFCCs:

<img src="https://github.com/NadimC137/SpeakerRecognitionKNN/blob/master/images/mfcc3.png" width="600">

Here, Filter Bank refers to the mel filters (converting to mel scale).After applying these, we get 26 coefficients (MFCC features). For any voice detection applications, only first 13 coefficients are important. So we keep the first 13 and discard the others.

