# Speaker Recognition using Pitch, MFCC and KNN classifier
This project uses machine learning approach to recognise speaker based on features extracted from recorded speech. Initial idea was taken from an [MathWorks article](https://www.mathworks.com/help/audio/examples/speaker-identification-using-pitch-and-mfcc.html) where they demonstrated the method using different MATLAB toolboxes, I avoided using those toolbox and tried to implement most of the algorithms from scratch.  

## Brief Introduction of the Algorithm:
I extracted features from voice which is fundamental to every speaker. Some of these features are pitch and MFCC (Mel-frequency cepstral coefficients). I extracted these features from voice data and stored them to use later for training in machine learning algorithm. The algorithm that was used in order to classify speaker is K nearest neighbor(KNN). This is a supervised classifier algorithm. Features extracted are used in KNN to make the training data. Then when new user input (speech) is given, KNN algorithm identifies the speaker based on the training data. 

<img src="https://www.mathworks.com/help/examples/audio_wavelet/win64/xxSpeakerID01.png" width="600">

### How KNN Algorithm Works:
Every supervised classifiers need some features for every sample data (training or testing). Let us say, a similar type of problem where whether a patient has cancer or not, has to be detected. Each patient data has two features by which we can decide whether he has cancer or not. Then we can plot every train data, where x and y axis represent two feature values. If the red samples are positive and green are negative then it should look something like below.


<img src="https://github.com/NadimC137/SpeakerRecognitionKNN/blob/master/images/knn1.png" width="400">

Now we want to detect a patient’s condition, whose data has been plotted using star sign. We intend to find out the class of the star. It can be green or red. The “K” in KNN algorithm is the nearest neighbors we wish to take vote from. Let’s say K = 3. Hence, we will now make a circle with the star as center just as big as to enclose only three data points on the plane. Refer to following diagram to get an idea:

<img src="https://github.com/NadimC137/SpeakerRecognitionKNN/blob/master/images/knn2.png" width="400">

The three closest points to the star is all Red. Hence, with good confidence level we can say that the star should belong to the class Red. That is, the patient has cancer. This is k nearest neighbor algorithm.


### Feature Extraction:
We will extract two features from voice. They are-
*	Pitch
*	MFCC (Mel-frequency cepstral coefficients).

### Pitch:
Pitch is frequency domain analysis of voice data, yet it is not exactly Fourier transform, because in pitch, we take a short time frame (window) and do the FFT of that portion, then go to next time frame and do the same thing. Thus we get a frequency response depending on time. This is important in our case, because we want to extract frequencies when speaker is talking and that should be variable frequency depending on time. Pitch analysis of voice data looks something like this:

<img src="https://github.com/NadimC137/SpeakerRecognitionKNN/blob/master/images/pitch1.png" width="800">

### MFCC (Mel-frequency cepstral coefficients):
The main point to understand about speech is that the sounds generated by a human are filtered by the shape of the vocal tract including tongue, teeth etc. This shape determines what sound comes out. If we can determine the shape accurately, this should give us an accurate representation of the phoneme being produced. The shape of the vocal tract manifests itself in the envelope of the short time power spectrum, and the job of MFCCs is to accurately represent this envelope. There is an amazing article about MFCC in medium [The dummy’s guide to MFCC](https://medium.com/prathena/the-dummys-guide-to-mfcc-aceab2450fd) which was very helpful when I first started with this project.

Cepstral is spectral with the spec reversed. There is a reason behind this naming. For a very basic understanding, cepstrum is the information of rate of change in spectral bands. In the conventional analysis of time signals, any periodic component shows up as sharp peaks in the corresponding frequency spectrum (Fourier spectrum). This is obtained by applying a Fourier transform on the time signal.
On taking the log of the magnitude of this Fourier spectrum, and then again taking the spectrum of this log by a cosine transformation, we observe a peak wherever there is a periodic element in the original time signal. Since we apply a transform on the frequency spectrum itself, the resulting spectrum is neither in the frequency domain nor in the time domain and hence it was decided to call this the quefrency domain. And this spectrum of the log of the spectrum of the time signal was named cepstrum. The following image is a summary of the above explained steps.

<img src="https://github.com/NadimC137/SpeakerRecognitionKNN/blob/master/images/mfcc1.png" width="600">

Pitch is one of the characteristics of a speech signal and is measured as the frequency of the signal. Mel scale is a scale that relates the perceived frequency of a tone to the actual measured frequency. It scales the frequency in order to match more closely what the human ear can hear (humans are better at identifying small changes in speech at lower frequencies). This scale has been derived from sets of experiments on human subjects. Let me give you an intuitive explanation of what the mel scale captures.
The range of human hearing is 20 Hz to 20 kHz. Imagine a tune at 300 Hz. This would sound something like the standard dialer tone of a land-line phone. Now imagine a tune at 400 Hz (a little higher pitched dialer tone). Now compare the distance between these two howsoever this may be perceived by your brain. Now imagine a 900 Hz signal (similar to a microphone feedback sound) and a 1 kHz sound. The perceived distance between these two sounds may seem greater than the first two although the actual difference is the same (100Hz). The mel scale tries to capture such differences. A frequency measured in Hertz (f) can be converted to the Mel scale using the following formula:

<img src="https://github.com/NadimC137/SpeakerRecognitionKNN/blob/master/images/mfcc2.jpg" width="400">

Any sound generated by humans is determined by the shape of their vocal tract (including tongue, teeth, etc.) If this shape can be determined correctly, any sound produced can be accurately represented. The envelope of the time power spectrum of the speech signal is representative of the vocal tract and MFCC (which is nothing but the coefficients that make up the Mel-frequency cepstrum) accurately represents this envelope. The following block diagram is a step-wise summary of how we arrived at MFCCs:

<img src="https://github.com/NadimC137/SpeakerRecognitionKNN/blob/master/images/mfcc3.png" width="600">

Here, Filter Bank refers to the mel filters (converting to mel scale).After applying these, we get 26 coefficients (MFCC features). For any voice detection applications, only first 13 coefficients are important. So we keep the first 13 and discard the others.


## Full Project Step by Step:
This project has two parts:
* Training Module: voice data were collected and processed to store them for training purpose.
* Testing Module: speaker was identified with the help of trained classifier.

### Part 1: Training Module:
In training module, all speakers voice data were recorded along with his name/user ID to extract features from their voice and then store them in an excel file, for later use (testing). All the steps of training module is explained step by step below-

#### Collecting voice data along with Speaker ID: 
We take voice samples from each of the volunteer speakers, and a user ID for each of them (Dhrubo=1, Nadim=2, Rafee=3). We collect voice data using Window 10’s default voice recorder for better quality of the voice data.


```matlab
% Taking User ID and voice data for training
delete C:\Users\User\Documents\'Sound recordings'\Recording.m4a
 
system('C:\Users\User\Desktop\Voice Recorder.lnk');
 
 
prompt = 'User No?:'; %dhrubo=1, nadim=2, rafee=3
user = input(prompt)
 
if user==1 || user==2 || user==3
   [audioIn,fs] = audioread('C:\Users\User\Documents\Sound recordings\Recording.m4a'); 
else
    return;
end
```
#### Defining Overlapping window:
We will take small portion of the signal at a time (frame) and perform calculation. Then we will go forward a little and calculate again. This kind of method is called overlapping window.


<img src="https://github.com/NadimC137/SpeakerRecognitionKNN/blob/master/images/overlap1.png" width="600">

Each frame will act as a training sample. In each frame we will perform speech detection, pitch and MFCC.

Here we have defined window length, overlap length and hop length-

```matlab
% defining framesize of samples  to extract features
windowlength = round(0.03*fs); % 30ms
overlaplength = round(0.02*fs); %20ms
hoplength = windowlength - overlaplength;
```

#### Speech Detection, Pitch and MFCC:
For each frame we need to detect if it a speech or non-speech data. If it is speech then we will do further calculation, otherwise not. For detecting speech steps are-
* Calculate average power of frame.
* If average value it higher than a threshold value, then its speech data.

This method is called short time average power algorithm. Then if the frame is speech data we calculate its pitch and MFCC.
Then we calculate pitch and MFCC for this frame. In Matlab 2018, we have built in functions for pitch and MFCC. So we used those instead of making user defined function.
Matlab code for these calculations-

```matlab
% extracting speech data from whole audio data (noise cancellation) and calculating pitch and MFCC
frame=zeros(windowlength,1);
audioIn2= zeros(length(audioIn),1);
j=0;
limit=0;
f1=[];
coeffs=[];
 
while limit<length(audioIn) 
   a=(j*hoplength)+1;
   b=a+windowlength;
   frame=audioIn(a:b,1);
   framesq=frame.^2;
   avgpow=sum(framesq)/windowlength;
   
   if avgpow> 1e-4
    audioIn2(a:b,1)=frame;
    fx = pitch(frame,fs, ...
    'WindowLength',windowlength, ...
    'OverlapLength',overlaplength, ...
    'Range',[50 400], ...
    'MedianFilterLength',3);
    coeffsx = mfcc(frame,fs,'LogEnergy','Replace');
    f1= [f1; fx];
    coeffs=[coeffs; coeffsx];
   end
   j=j+1;
   limit=(j*hoplength)+1+windowlength;
end
```

#### Exporting training data to excel file:
We merge together MFCC and pitch data for same frames and append user ID of that frame. We get a huge matrix of 15 columns, where 14 of them are features (MFCC and pitch) and last one is user ID. We save the data in excel file.

```matlab
%% combining pitch and MFCC and exporting total feature data to excel sheet
[m,n] = size(coeffs)
coeffs2=[coeffs f1 ones(m,1)*user]; %appending pitch with MFCC, to get 14 features together
 
filename = 'traindata.xlsx'; %import training excel file to append new samples   
past=xlsread(filename);
new=[past; coeffs2];
xlswrite(filename,new);
```

Excel data looks like this-

<img src="https://github.com/NadimC137/SpeakerRecognitionKNN/blob/master/images/exceldata.png" width="600">

### Part 2: Testing Module:
In testing module we will take speaker voice to detect the speaker. We will use previously stored speech data in training module and apply machine learning.
These steps below for testing module are almost exactly same as training module-
* Collecting voice data ( here we will not take user ID data because this is our task to find out)
* Defining Window length.
* Detecting speech data, calculating pitch and MFCC.

#### Feature Scaling:
Now we will Import Excel data that we stored in training phase. Now we have training data in excel file from training phase and test data. All features in both these data need feature scaling for better convergence. Formula and matlab code looks like below:

<img src="https://github.com/NadimC137/SpeakerRecognitionKNN/blob/master/images/eqn1.gif" width="200">

```matlab
%% loading training data to start machine learning algorithm (K nearest neighbor)
filename = 'traindata.xlsx';
train=xlsread(filename);
forscaling=[train; coeffs2];
 
%% performing feature scaling so that data converges
meanval= mean(forscaling);
stdval = std(forscaling,1);
 
[m,n] = size(train);
 
for j=1:n-1
    for i=1:m
        train(i,j)=(train(i,j)-meanval(j))/stdval(j);
    end
end
 
[p,q] = size(coeffs2);
 
for j=1:q-1
    for i=1:p
        coeffs2(i,j)=(coeffs2(i,j)-meanval(j))/stdval(j);
    end
end

```

#### KNN Algorithm Implementation:
After framing and calculating MFCC and pitch, we have got good number of frames as test data, each of them will be tested using KNN. Then we will take the most frequent result out of all test frame.
Steps applying KNN to any one frame are given below-

* Calculate the Euclidian distance between test sample frame and every training sample frame. Formula is-

 <img src="https://github.com/NadimC137/SpeakerRecognitionKNN/blob/master/images/eqn2.png" width="300">
 
*	Define the value of K, number of nearest neighbor that we are going to pick.
*	Add the distance and the index of the example to an ordered collection.
*	Sort the ordered collection of distances and indices from smallest to largest (in ascending order) by the distances.
*	Pick the first K entries from the sorted collection.
*	Get the user ID of the selected K entries.
*	Return the most frequent of the selected user IDs.

For all test samples we have to do these calculation and find their KNN prediction. Out of all those prediction, the most frequent prediction is selected as the user ID of the speaker.

We also implemented an optional feature, that if an unknown user speaks, then system will detect and will output 0 as ID. We took a threshold for Euclidian distance, if distance is more than threshold, voice data is considered to be of unknown user. 

Matlab implementation-

```matlab
% k nearest neighbor algorithm to detect speaker
 
sqdist=zeros(m,n-1,p);
 
for k=1:p
   for j=1:n-1
       for i=1:m
           sqdist(i,j,k)=(coeffs2(k,j)-train(i,j))^2;
       end
   end
end
 
sumval=zeros(m,p);
 
for k=1:p
    for i=1:m
       sumval(i,k)=sum(sqdist(i,:,k));
    end
end
 
 
 
kval=10;           
[b,ind] = mink(sumval,kval);
 
 
[u,v]=size(ind);
 
result=zeros(1,v);
resultmtx=zeros(u,v);
 
for j=1:v
    
    dhrubo=0; %user1
    nadim=0; %user2
    rafee=0; %user3
    
    for i=1:u
        indx=ind(i,j);
        resultmtx(i,j)= train(indx,15);
    end
    result(1,j)=mode(resultmtx(:,j));
    
end
        
 [final,freq] = mode(result); 
 accuracy=(freq/length(result))*100;
 disp(accuracy);
 disp(final)

```

### Further Improvements:
*	Wavelet transform can be used for speech detection and pitch detection. This will improve frame selection. 
*	More features can be extracted like delta and delta-delta. Delta is the derivative of MFCC and delta-delta is the derivative of delta.
*	Other supervised classifier machine learning algorithm can be applied to get improvement in this model. GMM (Gaussian Mixture Model) is one example for this.
